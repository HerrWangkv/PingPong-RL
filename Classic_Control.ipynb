{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('rl': conda)"
  },
  "interpreter": {
   "hash": "05488fbf6fc33f3e5c8e915a2948a13c2897205ee74ab29e243934251ee0787c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_without_cnn import TRPO, prepro_for_MountainCar, reward_for_MountainCar\n",
    "import datetime\n",
    "import gym\n",
    "import wandb\n",
    "import torch"
   ]
  },
  {
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.seed(42)\n",
    "file_dir = os.path.join(os.getcwd(), f'./{datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M\")}')\n",
    "os.makedirs(file_dir, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_episodes = 50000\n",
    "wandb.init(project='OtherRLEnv')\n",
    "trpo_agent = TRPO(env, file_dir, prepro_for_MountainCar, device, reward_for_MountainCar, threshold=-150, max_kl=0.05)\n",
    "trpo_agent.train(num_episodes) "
   ],
   "cell_type": "code",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaiwen\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.32<br/>\n                Syncing run <strong style=\"color:#cdcd00\">spring-elevator-67</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/kaiwen/OtherRLEnv\" target=\"_blank\">https://wandb.ai/kaiwen/OtherRLEnv</a><br/>\n                Run page: <a href=\"https://wandb.ai/kaiwen/OtherRLEnv/runs/3588jc3q\" target=\"_blank\">https://wandb.ai/kaiwen/OtherRLEnv/runs/3588jc3q</a><br/>\n                Run data is saved locally in <code>/home/kaiwen/Desktop/RL/PingPong-RL/wandb/run-20210703_014943-3588jc3q</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ": -91.0\n",
      "buffer size:91\n",
      "expected_return = 5856.4912109375\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 260.50103759765625\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 89 | total reward: -88.0\n",
      "buffer size:88\n",
      "expected_return = 7916.79736328125\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 322.45941162109375\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 90 | total reward: -99.0\n",
      "buffer size:99\n",
      "expected_return = 11433.9736328125\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 336.5993957519531\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 91 | total reward: -160.0\n",
      "buffer size:160\n",
      "expected_return = 15826.162109375\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 156.96470642089844\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 92 | total reward: -161.0\n",
      "buffer size:161\n",
      "expected_return = 22410.234375\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 184.7826690673828\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 93 | total reward: -91.0\n",
      "buffer size:91\n",
      "expected_return = 31506.892578125\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 276.5794982910156\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 94 | total reward: -93.0\n",
      "buffer size:93\n",
      "expected_return = 45027.05078125\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 347.1625061035156\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 95 | total reward: -167.0\n",
      "buffer size:167\n",
      "expected_return = 64736.8671875\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 211.5866241455078\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 96 | total reward: -102.0\n",
      "buffer size:102\n",
      "expected_return = 92628.2109375\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 349.8797302246094\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 97 | total reward: -88.0\n",
      "buffer size:88\n",
      "expected_return = 132333.5625\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 236.22674560546875\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 98 | total reward: -94.0\n",
      "buffer size:94\n",
      "expected_return = 190431.328125\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 356.332763671875\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 99 | total reward: -158.0\n",
      "buffer size:158\n",
      "expected_return = 273786.75\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 135.14219665527344\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 100 | total reward: -159.0\n",
      "buffer size:159\n",
      "expected_return = 393833.03125\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 158.918701171875\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 101 | total reward: -158.0\n",
      "buffer size:158\n",
      "expected_return = 566613.375\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 251.18753051757812\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 102 | total reward: -88.0\n",
      "buffer size:88\n",
      "expected_return = 815203.8125\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 272.8856506347656\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 103 | total reward: -92.0\n",
      "buffer size:92\n",
      "expected_return = 1173685.75\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 340.27337646484375\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 104 | total reward: -97.0\n",
      "buffer size:97\n",
      "expected_return = 1689867.75\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 345.982421875\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 105 | total reward: -123.0\n",
      "buffer size:123\n",
      "expected_return = 2433783.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 532.7235107421875\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 106 | total reward: -164.0\n",
      "buffer size:164\n",
      "expected_return = 3503007.75\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 232.14820861816406\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 107 | total reward: -165.0\n",
      "buffer size:165\n",
      "expected_return = 5043811.5\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 187.6697998046875\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 108 | total reward: -158.0\n",
      "buffer size:158\n",
      "expected_return = 7262354.5\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 156.79383850097656\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 109 | total reward: -158.0\n",
      "buffer size:158\n",
      "expected_return = 10457311.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 198.77976989746094\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 110 | total reward: -168.0\n",
      "buffer size:168\n",
      "expected_return = 15058330.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 218.58163452148438\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 111 | total reward: -165.0\n",
      "buffer size:165\n",
      "expected_return = 21683324.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 228.71678161621094\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 112 | total reward: -159.0\n",
      "buffer size:159\n",
      "expected_return = 31223236.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 174.0716094970703\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 113 | total reward: -173.0\n",
      "buffer size:173\n",
      "expected_return = 44961400.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 272.913330078125\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 114 | total reward: -165.0\n",
      "buffer size:165\n",
      "expected_return = 64743564.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 204.21209716796875\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 115 | total reward: -160.0\n",
      "buffer size:160\n",
      "expected_return = 93229984.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 127.43367767333984\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 116 | total reward: -95.0\n",
      "buffer size:95\n",
      "expected_return = 134250704.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 338.73736572265625\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 117 | total reward: -94.0\n",
      "buffer size:94\n",
      "expected_return = 193320512.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 315.6246643066406\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 118 | total reward: -161.0\n",
      "buffer size:161\n",
      "expected_return = 278381216.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 180.43434143066406\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 119 | total reward: -159.0\n",
      "buffer size:159\n",
      "expected_return = 400868384.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 167.06813049316406\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 120 | total reward: -162.0\n",
      "buffer size:162\n",
      "expected_return = 577250048.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 222.36196899414062\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 121 | total reward: -92.0\n",
      "buffer size:92\n",
      "expected_return = 831239424.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 331.1392517089844\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 122 | total reward: -160.0\n",
      "buffer size:160\n",
      "expected_return = 1196984448.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 147.5803985595703\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 123 | total reward: -160.0\n",
      "buffer size:160\n",
      "expected_return = 1723657088.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 126.64337158203125\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 124 | total reward: -161.0\n",
      "buffer size:161\n",
      "expected_return = 2482065664.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 218.48890686035156\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 125 | total reward: -172.0\n",
      "buffer size:172\n",
      "expected_return = 3574174464.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 260.107421875\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 126 | total reward: -161.0\n",
      "buffer size:161\n",
      "expected_return = 5146810368.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 220.03073120117188\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 127 | total reward: -89.0\n",
      "buffer size:89\n",
      "expected_return = 7411405824.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 268.449462890625\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 128 | total reward: -165.0\n",
      "buffer size:165\n",
      "expected_return = 10672424960.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 238.67471313476562\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 129 | total reward: -158.0\n",
      "buffer size:158\n",
      "expected_return = 15368290304.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 189.640380859375\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 130 | total reward: -159.0\n",
      "buffer size:159\n",
      "expected_return = 22130337792.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 191.23016357421875\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 131 | total reward: -159.0\n",
      "buffer size:159\n",
      "expected_return = 31867686912.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 143.78128051757812\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 132 | total reward: -158.0\n",
      "buffer size:158\n",
      "expected_return = 45889466368.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 146.4571075439453\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 133 | total reward: -158.0\n",
      "buffer size:158\n",
      "expected_return = 66080829440.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 132.01036071777344\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 134 | total reward: -88.0\n",
      "buffer size:88\n",
      "expected_return = 95156404224.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 253.01431274414062\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 135 | total reward: -160.0\n",
      "buffer size:160\n",
      "expected_return = 137025216512.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 191.1917724609375\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 136 | total reward: -112.0\n",
      "buffer size:112\n",
      "expected_return = 197316296704.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 507.3955078125\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 137 | total reward: -175.0\n",
      "buffer size:175\n",
      "expected_return = 284135489536.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 218.98880004882812\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 138 | total reward: -160.0\n",
      "buffer size:160\n",
      "expected_return = 409155108864.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 157.5850372314453\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 139 | total reward: -169.0\n",
      "buffer size:169\n",
      "expected_return = 589183385600.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 237.39617919921875\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 140 | total reward: -161.0\n",
      "buffer size:161\n",
      "expected_return = 848423944192.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 196.6997833251953\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 141 | total reward: -160.0\n",
      "buffer size:160\n",
      "expected_return = 1221730631680.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 191.79098510742188\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 142 | total reward: -102.0\n",
      "buffer size:102\n",
      "expected_return = 1759291899904.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 363.0103759765625\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 143 | total reward: -91.0\n",
      "buffer size:91\n",
      "expected_return = 2533380325376.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 260.25537109375\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 144 | total reward: -89.0\n",
      "buffer size:89\n",
      "expected_return = 3648067993600.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 236.54698181152344\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 145 | total reward: -164.0\n",
      "buffer size:164\n",
      "expected_return = 5253217910784.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 214.05088806152344\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 146 | total reward: -91.0\n",
      "buffer size:91\n",
      "expected_return = 7564634357760.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 259.145263671875\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 147 | total reward: -166.0\n",
      "buffer size:166\n",
      "expected_return = 10893072007168.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 210.85659790039062\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 148 | total reward: -91.0\n",
      "buffer size:91\n",
      "expected_return = 15686023774208.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 286.7085876464844\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 149 | total reward: -159.0\n",
      "buffer size:159\n",
      "expected_return = 22587872641024.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 224.84228515625\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 150 | total reward: -88.0\n",
      "buffer size:88\n",
      "expected_return = 32526538113024.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 248.41006469726562\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 151 | total reward: -103.0\n",
      "buffer size:103\n",
      "expected_return = 46838216392704.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 392.15960693359375\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 152 | total reward: -160.0\n",
      "buffer size:160\n",
      "expected_return = 67447029760000.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 192.05445861816406\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 153 | total reward: -158.0\n",
      "buffer size:158\n",
      "expected_return = 97123722854400.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 218.62123107910156\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 154 | total reward: -158.0\n",
      "buffer size:158\n",
      "expected_return = 139858177687552.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 203.49609375\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 155 | total reward: -159.0\n",
      "buffer size:159\n",
      "expected_return = 201395764461568.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 238.29115295410156\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 156 | total reward: -161.0\n",
      "buffer size:161\n",
      "expected_return = 290009915588608.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 176.77291870117188\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 157 | total reward: -160.0\n",
      "buffer size:160\n",
      "expected_return = 417614232813568.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 174.9353485107422\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 158 | total reward: -160.0\n",
      "buffer size:160\n",
      "expected_return = 601364476461056.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 198.98727416992188\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 159 | total reward: -94.0\n",
      "buffer size:94\n",
      "expected_return = 865964996427776.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 326.75146484375\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 160 | total reward: -160.0\n",
      "buffer size:160\n",
      "expected_return = 1246989530431488.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 180.5617218017578\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 161 | total reward: -88.0\n",
      "buffer size:88\n",
      "expected_return = 1795664891609088.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 249.23251342773438\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 162 | total reward: -158.0\n",
      "buffer size:158\n",
      "expected_return = 2585757411704832.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 164.0808868408203\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 163 | total reward: -103.0\n",
      "buffer size:103\n",
      "expected_return = 3723490490318848.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 367.720947265625\n",
      "action_probs:  tensor([0., 0., 1.], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "Episode: 164 | total reward: -97.0\n",
      "buffer size:97\n",
      "expected_return = 5361825898037248.0\n",
      "diff = 0.0, new_kl = 0.0\n",
      "success, alpha = 1, diff=0.0, kl_mean=0.0\n",
      "value_loss = 369.4203186035156\n",
      "*********FINISHED*********\n"
     ]
    }
   ]
  },
  {
   "source": [
    "import gym\n",
    "import os\n",
    "import torch\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trpo_agent = TRPO(env, device, prepro_for_MountainCar)\n",
    "file_dir = os.path.join(os.getcwd(), 'MountainCar/checkpoint_BEST')\n",
    "trpo_agent.load_policy(os.path.join(file_dir, 'policy.pt'))\n",
    "trpo_agent.load_value(os.path.join(file_dir, 'value.pt'))\n",
    "trpo_agent.show(5)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode: 0 | total reward: -165.0\n",
      "Episode: 1 | total reward: -89.0\n",
      "Episode: 2 | total reward: -164.0\n",
      "Episode: 3 | total reward: -171.0\n",
      "Episode: 4 | total reward: -100.0\n"
     ]
    }
   ]
  }
 ]
}